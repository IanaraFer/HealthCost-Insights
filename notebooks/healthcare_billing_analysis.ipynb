{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c757c426",
   "metadata": {},
   "source": [
    "# HealthCost Insights: Healthcare Billing Anomaly Detection ðŸ¥ðŸ’°\n",
    "\n",
    "## Project Overview\n",
    "This notebook provides comprehensive analysis of healthcare billing data to detect anomalies, identify cost patterns, and prepare insights for Power BI dashboards. The analysis includes:\n",
    "\n",
    "- **Data Generation**: Creating realistic healthcare billing datasets\n",
    "- **Exploratory Analysis**: Understanding data patterns and distributions  \n",
    "- **Anomaly Detection**: Statistical and ML-based approaches to identify billing irregularities\n",
    "- **Cost Analysis**: Deep dive into cost patterns and trends\n",
    "- **Visualization**: Interactive charts and plots for insights\n",
    "- **Export Preparation**: Clean data export for Power BI integration\n",
    "\n",
    "## Business Value\n",
    "Healthcare billing anomaly detection can help organizations:\n",
    "- ðŸ” Identify potential fraud and billing errors\n",
    "- ðŸ’¡ Optimize cost management and resource allocation\n",
    "- ðŸ“Š Improve financial transparency and compliance\n",
    "- âš¡ Enhance operational efficiency in revenue cycle management\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cba787",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries ðŸ“š\n",
    "\n",
    "Importing essential libraries for data manipulation, analysis, visualization, and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7929d402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Machine learning and statistical analysis\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Utility libraries\n",
    "import warnings\n",
    "import os\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "fake = Faker()\n",
    "Faker.seed(42)\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "print(f\"ðŸ“Š Pandas version: {pd.__version__}\")\n",
    "print(f\"ðŸ”¢ NumPy version: {np.__version__}\")\n",
    "print(f\"ðŸ“ˆ Matplotlib version: {plt.matplotlib.__version__}\")\n",
    "print(f\"ðŸ¤– Scikit-learn version: {sklearn.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1281e826",
   "metadata": {},
   "source": [
    "## 2. Generate Simulated Healthcare Billing Dataset ðŸ¥\n",
    "\n",
    "Creating a comprehensive synthetic healthcare billing dataset with realistic features and intentional anomalies for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af762f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_healthcare_billing_data(num_records=50000):\n",
    "    \"\"\"\n",
    "    Generate comprehensive healthcare billing dataset with realistic anomalies\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ”§ Generating {num_records:,} healthcare billing records...\")\n",
    "    \n",
    "    # Define realistic medical procedures with base costs and frequencies\n",
    "    procedures = {\n",
    "        'Emergency Room Visit': {'base_cost': 1200, 'variance': 400, 'frequency': 0.15},\n",
    "        'Routine Checkup': {'base_cost': 250, 'variance': 50, 'frequency': 0.25},\n",
    "        'Blood Test': {'base_cost': 150, 'variance': 30, 'frequency': 0.20},\n",
    "        'X-Ray': {'base_cost': 300, 'variance': 75, 'frequency': 0.12},\n",
    "        'MRI Scan': {'base_cost': 2500, 'variance': 500, 'frequency': 0.05},\n",
    "        'CT Scan': {'base_cost': 1800, 'variance': 300, 'frequency': 0.08},\n",
    "        'Surgery - Minor': {'base_cost': 5000, 'variance': 1000, 'frequency': 0.04},\n",
    "        'Surgery - Major': {'base_cost': 25000, 'variance': 8000, 'frequency': 0.02},\n",
    "        'Physical Therapy': {'base_cost': 180, 'variance': 40, 'frequency': 0.09}\n",
    "    }\n",
    "    \n",
    "    # Insurance providers with coverage characteristics\n",
    "    insurance_providers = {\n",
    "        'BlueCross BlueShield': {'coverage_rate': 0.80, 'frequency': 0.25},\n",
    "        'Aetna': {'coverage_rate': 0.75, 'frequency': 0.20},\n",
    "        'UnitedHealth': {'coverage_rate': 0.82, 'frequency': 0.22},\n",
    "        'Cigna': {'coverage_rate': 0.78, 'frequency': 0.15},\n",
    "        'Medicare': {'coverage_rate': 0.85, 'frequency': 0.10},\n",
    "        'Medicaid': {'coverage_rate': 0.90, 'frequency': 0.08}\n",
    "    }\n",
    "    \n",
    "    # Medical departments and specialties\n",
    "    departments = [\n",
    "        'Emergency Medicine', 'Internal Medicine', 'Cardiology', 'Orthopedics',\n",
    "        'Radiology', 'Surgery', 'Pediatrics', 'Neurology', 'Oncology', 'Psychiatry'\n",
    "    ]\n",
    "    \n",
    "    # Generate patient demographics and billing data\n",
    "    data = []\n",
    "    \n",
    "    for i in range(num_records):\n",
    "        # Patient information\n",
    "        patient_id = f\"P{10000 + i:06d}\"\n",
    "        patient_age = max(1, min(95, int(np.random.normal(45, 18))))\n",
    "        patient_gender = np.random.choice(['Male', 'Female', 'Other'], p=[0.48, 0.50, 0.02])\n",
    "        \n",
    "        # Procedure selection based on realistic frequencies\n",
    "        procedure_name = np.random.choice(\n",
    "            list(procedures.keys()),\n",
    "            p=[procedures[p]['frequency'] for p in procedures.keys()]\n",
    "        )\n",
    "        procedure_info = procedures[procedure_name]\n",
    "        \n",
    "        # Calculate realistic billing amounts\n",
    "        base_cost = max(50, np.random.normal(\n",
    "            procedure_info['base_cost'], \n",
    "            procedure_info['variance']\n",
    "        ))\n",
    "        \n",
    "        # Age-based cost adjustment (older patients often have higher costs)\n",
    "        age_multiplier = 1.0 + (patient_age - 30) * 0.005 if patient_age > 30 else 1.0\n",
    "        base_cost *= age_multiplier\n",
    "        \n",
    "        # Insurance selection and payment calculation\n",
    "        insurance_name = np.random.choice(\n",
    "            list(insurance_providers.keys()),\n",
    "            p=[insurance_providers[p]['frequency'] for p in insurance_providers.keys()]\n",
    "        )\n",
    "        coverage_rate = insurance_providers[insurance_name]['coverage_rate']\n",
    "        \n",
    "        # Calculate payment amounts with realistic variation\n",
    "        total_billed = base_cost\n",
    "        insurance_paid = total_billed * coverage_rate * np.random.uniform(0.85, 1.0)\n",
    "        patient_responsibility = max(0, total_billed - insurance_paid)\n",
    "        \n",
    "        # Generate service dates (last 24 months)\n",
    "        start_date = datetime.now() - timedelta(days=730)\n",
    "        service_date = start_date + timedelta(days=random.randint(0, 730))\n",
    "        \n",
    "        # Additional realistic fields\n",
    "        department = random.choice(departments)\n",
    "        provider_id = f\"DR{random.randint(1000, 9999)}\"\n",
    "        \n",
    "        # Diagnosis codes (simplified ICD-10 style)\n",
    "        diagnosis_codes = [\n",
    "            'Z00.00', 'I10', 'E11.9', 'M79.1', 'R53.83', 'K21.9',\n",
    "            'F41.1', 'M25.511', 'N39.0', 'R50.9', 'H52.4', 'J06.9'\n",
    "        ]\n",
    "        primary_diagnosis = random.choice(diagnosis_codes)\n",
    "        \n",
    "        # Admission and length of stay logic\n",
    "        if 'Surgery' in procedure_name or 'Emergency' in procedure_name:\n",
    "            admission_type = np.random.choice(['Inpatient', 'Emergency'], p=[0.7, 0.3])\n",
    "            length_of_stay = random.randint(1, 10)\n",
    "        else:\n",
    "            admission_type = 'Outpatient'\n",
    "            length_of_stay = 1\n",
    "        \n",
    "        data.append({\n",
    "            'claim_id': f\"CLM{20240000 + i:08d}\",\n",
    "            'patient_id': patient_id,\n",
    "            'patient_age': patient_age,\n",
    "            'patient_gender': patient_gender,\n",
    "            'service_date': service_date.strftime('%Y-%m-%d'),\n",
    "            'procedure_name': procedure_name,\n",
    "            'procedure_code': f\"CPT{random.randint(10000, 99999)}\",\n",
    "            'primary_diagnosis': primary_diagnosis,\n",
    "            'department': department,\n",
    "            'provider_id': provider_id,\n",
    "            'insurance_provider': insurance_name,\n",
    "            'total_billed_amount': round(total_billed, 2),\n",
    "            'insurance_paid_amount': round(insurance_paid, 2),\n",
    "            'patient_responsibility': round(patient_responsibility, 2),\n",
    "            'claim_status': np.random.choice(['Paid', 'Pending', 'Denied'], p=[0.85, 0.10, 0.05]),\n",
    "            'admission_type': admission_type,\n",
    "            'length_of_stay': length_of_stay,\n",
    "            'facility_type': np.random.choice(['Hospital', 'Clinic', 'Urgent Care'], p=[0.6, 0.3, 0.1])\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Introduce realistic anomalies (approximately 5% of data)\n",
    "    print(\"ðŸš¨ Introducing realistic billing anomalies...\")\n",
    "    anomaly_count = int(num_records * 0.05)\n",
    "    anomaly_indices = np.random.choice(df.index, anomaly_count, replace=False)\n",
    "    \n",
    "    anomaly_types = []\n",
    "    \n",
    "    for idx in anomaly_indices:\n",
    "        anomaly_type = np.random.choice([\n",
    "            'billing_error', 'duplicate_claim', 'unusual_cost', 'fraud_pattern'\n",
    "        ], p=[0.3, 0.25, 0.25, 0.2])\n",
    "        \n",
    "        anomaly_types.append(anomaly_type)\n",
    "        \n",
    "        if anomaly_type == 'billing_error':\n",
    "            # Billing calculation errors - inflate costs unreasonably\n",
    "            df.loc[idx, 'total_billed_amount'] *= np.random.uniform(3.0, 8.0)\n",
    "            \n",
    "        elif anomaly_type == 'duplicate_claim':\n",
    "            # Create potential duplicate claims\n",
    "            if idx < len(df) - 1:\n",
    "                df.loc[idx + 1, 'patient_id'] = df.loc[idx, 'patient_id']\n",
    "                df.loc[idx + 1, 'procedure_name'] = df.loc[idx, 'procedure_name']\n",
    "                df.loc[idx + 1, 'service_date'] = df.loc[idx, 'service_date']\n",
    "                df.loc[idx + 1, 'provider_id'] = df.loc[idx, 'provider_id']\n",
    "                \n",
    "        elif anomaly_type == 'unusual_cost':\n",
    "            # Routine procedures with unusually high costs\n",
    "            if 'Routine' in df.loc[idx, 'procedure_name'] or 'Blood Test' in df.loc[idx, 'procedure_name']:\n",
    "                df.loc[idx, 'total_billed_amount'] *= np.random.uniform(15.0, 30.0)\n",
    "                \n",
    "        elif anomaly_type == 'fraud_pattern':\n",
    "            # Potential fraud indicators - multiple expensive procedures\n",
    "            df.loc[idx, 'procedure_name'] = 'Surgery - Major'\n",
    "            df.loc[idx, 'total_billed_amount'] = np.random.uniform(80000, 150000)\n",
    "            df.loc[idx, 'length_of_stay'] = random.randint(15, 30)\n",
    "    \n",
    "    # Add calculated fields for analysis\n",
    "    df['payment_rate'] = df['insurance_paid_amount'] / df['total_billed_amount']\n",
    "    df['cost_per_day'] = df['total_billed_amount'] / df['length_of_stay']\n",
    "    df['service_month'] = pd.to_datetime(df['service_date']).dt.to_period('M').astype(str)\n",
    "    df['service_quarter'] = pd.to_datetime(df['service_date']).dt.quarter\n",
    "    df['service_year'] = pd.to_datetime(df['service_date']).dt.year\n",
    "    df['day_of_week'] = pd.to_datetime(df['service_date']).dt.day_name()\n",
    "    \n",
    "    print(f\"âœ… Generated {len(df):,} healthcare billing records with {anomaly_count:,} anomalies\")\n",
    "    return df\n",
    "\n",
    "# Generate the dataset\n",
    "healthcare_df = generate_healthcare_billing_data(50000)\n",
    "\n",
    "print(f\"\\nðŸ“‹ Dataset Shape: {healthcare_df.shape}\")\n",
    "print(f\"ðŸ“… Date Range: {healthcare_df['service_date'].min()} to {healthcare_df['service_date'].max()}\")\n",
    "print(f\"ðŸ’° Total Billed: ${healthcare_df['total_billed_amount'].sum():,.2f}\")\n",
    "print(f\"ðŸ‘¥ Unique Patients: {healthcare_df['patient_id'].nunique():,}\")\n",
    "print(f\"ðŸ‘¨â€âš•ï¸ Unique Providers: {healthcare_df['provider_id'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228e0be0",
   "metadata": {},
   "source": [
    "## 3. Data Exploration and Profiling ðŸ”\n",
    "\n",
    "Comprehensive exploratory data analysis to understand the dataset structure, quality, and initial patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2470ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset information\n",
    "print(\"ðŸ“Š DATASET OVERVIEW\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Dataset Shape: {healthcare_df.shape}\")\n",
    "print(f\"Memory Usage: {healthcare_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"Duplicate Rows: {healthcare_df.duplicated().sum():,}\")\n",
    "\n",
    "# Data types and missing values\n",
    "print(\"\\nðŸ” DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 50)\n",
    "info_df = pd.DataFrame({\n",
    "    'Column': healthcare_df.columns,\n",
    "    'Data_Type': healthcare_df.dtypes,\n",
    "    'Non_Null_Count': healthcare_df.count(),\n",
    "    'Null_Count': healthcare_df.isnull().sum(),\n",
    "    'Null_Percentage': (healthcare_df.isnull().sum() / len(healthcare_df) * 100).round(2),\n",
    "    'Unique_Values': healthcare_df.nunique()\n",
    "})\n",
    "print(info_df)\n",
    "\n",
    "# Statistical summary for numerical columns\n",
    "print(\"\\nðŸ“ˆ NUMERICAL COLUMNS STATISTICS\")\n",
    "print(\"=\" * 50)\n",
    "numerical_cols = ['patient_age', 'total_billed_amount', 'insurance_paid_amount', \n",
    "                 'patient_responsibility', 'length_of_stay', 'cost_per_day', 'payment_rate']\n",
    "print(healthcare_df[numerical_cols].describe())\n",
    "\n",
    "# Categorical columns overview\n",
    "print(\"\\nðŸ“‹ CATEGORICAL COLUMNS OVERVIEW\")\n",
    "print(\"=\" * 50)\n",
    "categorical_cols = ['procedure_name', 'insurance_provider', 'department', \n",
    "                   'claim_status', 'admission_type', 'patient_gender']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n{col.upper()}:\")\n",
    "    print(healthcare_df[col].value_counts().head())\n",
    "\n",
    "# Display sample records\n",
    "print(\"\\nðŸ“ SAMPLE RECORDS\")\n",
    "print(\"=\" * 50)\n",
    "print(healthcare_df.head())\n",
    "\n",
    "# Check for potential data quality issues\n",
    "print(\"\\nâš ï¸ POTENTIAL DATA QUALITY ISSUES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Negative amounts\n",
    "negative_amounts = healthcare_df[healthcare_df['total_billed_amount'] < 0]\n",
    "print(f\"Records with negative billing amounts: {len(negative_amounts)}\")\n",
    "\n",
    "# Zero amounts\n",
    "zero_amounts = healthcare_df[healthcare_df['total_billed_amount'] == 0]\n",
    "print(f\"Records with zero billing amounts: {len(zero_amounts)}\")\n",
    "\n",
    "# Payment rate anomalies (>100% or <0%)\n",
    "payment_anomalies = healthcare_df[\n",
    "    (healthcare_df['payment_rate'] > 1.0) | (healthcare_df['payment_rate'] < 0)\n",
    "]\n",
    "print(f\"Records with unusual payment rates: {len(payment_anomalies)}\")\n",
    "\n",
    "# Very high billing amounts (potential outliers)\n",
    "high_amounts = healthcare_df[healthcare_df['total_billed_amount'] > 100000]\n",
    "print(f\"Records with billing amounts > $100,000: {len(high_amounts)}\")\n",
    "\n",
    "print(f\"\\nâœ… Data exploration completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656a9b6b",
   "metadata": {},
   "source": [
    "## 4. Data Cleaning and Preprocessing ðŸ§¹\n",
    "\n",
    "Cleaning the dataset and creating additional features for enhanced analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62ec9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a working copy for cleaning\n",
    "df_clean = healthcare_df.copy()\n",
    "\n",
    "print(\"ðŸ§¹ STARTING DATA CLEANING PROCESS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Initial dataset size: {df_clean.shape}\")\n",
    "\n",
    "# 1. Handle missing values (if any)\n",
    "print(f\"\\nMissing values before cleaning: {df_clean.isnull().sum().sum()}\")\n",
    "\n",
    "# 2. Remove records with negative or zero billing amounts\n",
    "initial_count = len(df_clean)\n",
    "df_clean = df_clean[df_clean['total_billed_amount'] > 0]\n",
    "removed_count = initial_count - len(df_clean)\n",
    "print(f\"Removed {removed_count} records with invalid billing amounts\")\n",
    "\n",
    "# 3. Cap extremely high billing amounts (potential outliers)\n",
    "# Using 99.5th percentile as threshold\n",
    "billing_threshold = df_clean['total_billed_amount'].quantile(0.995)\n",
    "extreme_outliers = df_clean[df_clean['total_billed_amount'] > billing_threshold]\n",
    "print(f\"Found {len(extreme_outliers)} extreme outliers (>${billing_threshold:,.2f})\")\n",
    "\n",
    "# 4. Convert date columns to datetime\n",
    "df_clean['service_date'] = pd.to_datetime(df_clean['service_date'])\n",
    "\n",
    "# 5. Create additional derived features for analysis\n",
    "print(f\"\\nðŸ”§ CREATING DERIVED FEATURES\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Age groups for demographic analysis\n",
    "def categorize_age(age):\n",
    "    if age < 18:\n",
    "        return 'Pediatric (0-17)'\n",
    "    elif age < 35:\n",
    "        return 'Young Adult (18-34)'\n",
    "    elif age < 50:\n",
    "        return 'Middle Age (35-49)'\n",
    "    elif age < 65:\n",
    "        return 'Older Adult (50-64)'\n",
    "    else:\n",
    "        return 'Senior (65+)'\n",
    "\n",
    "df_clean['age_group'] = df_clean['patient_age'].apply(categorize_age)\n",
    "\n",
    "# Cost categories\n",
    "def categorize_cost(amount):\n",
    "    if amount < 500:\n",
    "        return 'Low Cost (<$500)'\n",
    "    elif amount < 2000:\n",
    "        return 'Medium Cost ($500-$2,000)'\n",
    "    elif amount < 10000:\n",
    "        return 'High Cost ($2,000-$10,000)'\n",
    "    else:\n",
    "        return 'Very High Cost (>$10,000)'\n",
    "\n",
    "df_clean['cost_category'] = df_clean['total_billed_amount'].apply(categorize_cost)\n",
    "\n",
    "# Procedure complexity based on cost\n",
    "df_clean['procedure_complexity'] = df_clean['procedure_name'].map({\n",
    "    'Emergency Room Visit': 'High',\n",
    "    'Routine Checkup': 'Low',\n",
    "    'Blood Test': 'Low',\n",
    "    'X-Ray': 'Medium',\n",
    "    'MRI Scan': 'High',\n",
    "    'CT Scan': 'High',\n",
    "    'Surgery - Minor': 'High',\n",
    "    'Surgery - Major': 'Very High',\n",
    "    'Physical Therapy': 'Medium'\n",
    "})\n",
    "\n",
    "# Days between service and today (recency)\n",
    "df_clean['days_since_service'] = (pd.Timestamp.now() - df_clean['service_date']).dt.days\n",
    "\n",
    "# Insurance efficiency (how much insurance covers)\n",
    "df_clean['insurance_efficiency'] = df_clean['insurance_paid_amount'] / df_clean['total_billed_amount']\n",
    "\n",
    "# Weekend indicator\n",
    "df_clean['is_weekend'] = df_clean['service_date'].dt.weekday >= 5\n",
    "\n",
    "# Monthly billing totals per patient\n",
    "monthly_patient_totals = df_clean.groupby(['patient_id', 'service_month'])['total_billed_amount'].sum().reset_index()\n",
    "monthly_patient_totals.columns = ['patient_id', 'service_month', 'monthly_patient_total']\n",
    "df_clean = df_clean.merge(monthly_patient_totals, on=['patient_id', 'service_month'], how='left')\n",
    "\n",
    "# Provider billing statistics\n",
    "provider_stats = df_clean.groupby('provider_id').agg({\n",
    "    'total_billed_amount': ['count', 'mean', 'sum'],\n",
    "    'patient_id': 'nunique'\n",
    "}).round(2)\n",
    "provider_stats.columns = ['provider_claim_count', 'provider_avg_billing', 'provider_total_billing', 'provider_unique_patients']\n",
    "provider_stats = provider_stats.reset_index()\n",
    "df_clean = df_clean.merge(provider_stats, on='provider_id', how='left')\n",
    "\n",
    "print(f\"âœ… Added {len([col for col in df_clean.columns if col not in healthcare_df.columns])} new derived features\")\n",
    "\n",
    "# 6. Identify potential duplicate claims\n",
    "print(f\"\\nðŸ” IDENTIFYING POTENTIAL DUPLICATE CLAIMS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Define criteria for potential duplicates\n",
    "duplicate_criteria = ['patient_id', 'procedure_name', 'service_date', 'provider_id']\n",
    "potential_duplicates = df_clean[df_clean.duplicated(subset=duplicate_criteria, keep=False)]\n",
    "print(f\"Found {len(potential_duplicates)} potential duplicate claims\")\n",
    "\n",
    "# Flag duplicates without removing them\n",
    "df_clean['is_potential_duplicate'] = df_clean.duplicated(subset=duplicate_criteria, keep=False)\n",
    "\n",
    "# 7. Create final summary\n",
    "print(f\"\\nðŸ“Š FINAL DATASET SUMMARY\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Final dataset size: {df_clean.shape}\")\n",
    "print(f\"Total features: {len(df_clean.columns)}\")\n",
    "print(f\"Date range: {df_clean['service_date'].min().date()} to {df_clean['service_date'].max().date()}\")\n",
    "print(f\"Total billing amount: ${df_clean['total_billed_amount'].sum():,.2f}\")\n",
    "print(f\"Average claim amount: ${df_clean['total_billed_amount'].mean():,.2f}\")\n",
    "print(f\"Potential duplicates flagged: {df_clean['is_potential_duplicate'].sum():,}\")\n",
    "\n",
    "print(f\"\\nâœ… Data cleaning and preprocessing completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11fbae8",
   "metadata": {},
   "source": [
    "## 5. Statistical Anomaly Detection ðŸ“Š\n",
    "\n",
    "Implementing statistical methods to identify billing anomalies and outliers using Z-score, IQR, and percentile-based approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8fd7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_statistical_anomalies(df, target_column='total_billed_amount'):\n",
    "    \"\"\"\n",
    "    Detect anomalies using multiple statistical methods\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ” STATISTICAL ANOMALY DETECTION FOR: {target_column}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    anomaly_results = {}\n",
    "    \n",
    "    # 1. Z-Score Method (threshold: |z| > 3)\n",
    "    z_scores = np.abs(zscore(df[target_column]))\n",
    "    z_anomalies = z_scores > 3\n",
    "    anomaly_results['z_score'] = z_anomalies\n",
    "    \n",
    "    print(f\"ðŸ“Š Z-Score Method (|z| > 3):\")\n",
    "    print(f\"   Anomalies detected: {z_anomalies.sum():,} ({z_anomalies.mean()*100:.2f}%)\")\n",
    "    print(f\"   Z-score range: {z_scores.min():.2f} to {z_scores.max():.2f}\")\n",
    "    \n",
    "    # 2. Interquartile Range (IQR) Method\n",
    "    Q1 = df[target_column].quantile(0.25)\n",
    "    Q3 = df[target_column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    iqr_anomalies = (df[target_column] < lower_bound) | (df[target_column] > upper_bound)\n",
    "    anomaly_results['iqr'] = iqr_anomalies\n",
    "    \n",
    "    print(f\"\\nðŸ“¦ IQR Method (1.5 Ã— IQR):\")\n",
    "    print(f\"   Lower bound: ${lower_bound:,.2f}\")\n",
    "    print(f\"   Upper bound: ${upper_bound:,.2f}\")\n",
    "    print(f\"   Anomalies detected: {iqr_anomalies.sum():,} ({iqr_anomalies.mean()*100:.2f}%)\")\n",
    "    \n",
    "    # 3. Percentile Method (99th percentile)\n",
    "    percentile_99 = df[target_column].quantile(0.99)\n",
    "    percentile_1 = df[target_column].quantile(0.01)\n",
    "    \n",
    "    percentile_anomalies = (df[target_column] > percentile_99) | (df[target_column] < percentile_1)\n",
    "    anomaly_results['percentile'] = percentile_anomalies\n",
    "    \n",
    "    print(f\"\\nðŸ“ˆ Percentile Method (1st/99th percentiles):\")\n",
    "    print(f\"   1st percentile: ${percentile_1:,.2f}\")\n",
    "    print(f\"   99th percentile: ${percentile_99:,.2f}\")\n",
    "    print(f\"   Anomalies detected: {percentile_anomalies.sum():,} ({percentile_anomalies.mean()*100:.2f}%)\")\n",
    "    \n",
    "    # 4. Modified Z-Score (using median absolute deviation)\n",
    "    median = df[target_column].median()\n",
    "    mad = np.median(np.abs(df[target_column] - median))\n",
    "    modified_z_scores = 0.6745 * (df[target_column] - median) / mad\n",
    "    modified_z_anomalies = np.abs(modified_z_scores) > 3.5\n",
    "    anomaly_results['modified_z'] = modified_z_anomalies\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Modified Z-Score Method (MAD-based):\")\n",
    "    print(f\"   Median: ${median:,.2f}\")\n",
    "    print(f\"   MAD: ${mad:,.2f}\")\n",
    "    print(f\"   Anomalies detected: {modified_z_anomalies.sum():,} ({modified_z_anomalies.mean()*100:.2f}%)\")\n",
    "    \n",
    "    # Combine all methods - consensus approach\n",
    "    anomaly_counts = sum(anomaly_results.values())\n",
    "    consensus_anomalies = anomaly_counts >= 2  # At least 2 methods agree\n",
    "    anomaly_results['consensus'] = consensus_anomalies\n",
    "    \n",
    "    print(f\"\\nðŸ¤ Consensus Method (â‰¥2 methods agree):\")\n",
    "    print(f\"   Anomalies detected: {consensus_anomalies.sum():,} ({consensus_anomalies.mean()*100:.2f}%)\")\n",
    "    \n",
    "    return anomaly_results\n",
    "\n",
    "# Apply statistical anomaly detection to billing amounts\n",
    "billing_anomalies = detect_statistical_anomalies(df_clean, 'total_billed_amount')\n",
    "\n",
    "# Add anomaly flags to dataframe\n",
    "for method, anomalies in billing_anomalies.items():\n",
    "    df_clean[f'anomaly_{method}'] = anomalies\n",
    "\n",
    "# Analyze anomalies by procedure type\n",
    "print(f\"\\nðŸ”¬ ANOMALY ANALYSIS BY PROCEDURE TYPE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "anomaly_by_procedure = df_clean[df_clean['anomaly_consensus']].groupby('procedure_name').agg({\n",
    "    'claim_id': 'count',\n",
    "    'total_billed_amount': ['mean', 'max'],\n",
    "    'insurance_paid_amount': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "anomaly_by_procedure.columns = ['anomaly_count', 'avg_amount', 'max_amount', 'avg_insurance_paid']\n",
    "anomaly_by_procedure = anomaly_by_procedure.sort_values('anomaly_count', ascending=False)\n",
    "print(anomaly_by_procedure)\n",
    "\n",
    "# Analyze anomalies by provider\n",
    "print(f\"\\nðŸ‘¨â€âš•ï¸ TOP PROVIDERS WITH ANOMALIES\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "provider_anomalies = df_clean[df_clean['anomaly_consensus']].groupby('provider_id').agg({\n",
    "    'claim_id': 'count',\n",
    "    'total_billed_amount': 'sum',\n",
    "    'patient_id': 'nunique'\n",
    "}).sort_values('claim_id', ascending=False).head(10)\n",
    "\n",
    "provider_anomalies.columns = ['anomaly_claims', 'total_anomaly_amount', 'unique_patients']\n",
    "print(provider_anomalies)\n",
    "\n",
    "# Temporal analysis of anomalies\n",
    "print(f\"\\nðŸ“… TEMPORAL PATTERN OF ANOMALIES\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "temporal_anomalies = df_clean[df_clean['anomaly_consensus']].groupby('service_month').agg({\n",
    "    'claim_id': 'count',\n",
    "    'total_billed_amount': 'sum'\n",
    "}).sort_values('service_month')\n",
    "\n",
    "temporal_anomalies.columns = ['monthly_anomaly_count', 'monthly_anomaly_amount']\n",
    "print(temporal_anomalies.tail(10))\n",
    "\n",
    "print(f\"\\nâœ… Statistical anomaly detection completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebace6d2",
   "metadata": {},
   "source": [
    "## 6. Machine Learning-Based Anomaly Detection ðŸ¤–\n",
    "\n",
    "Applying advanced machine learning algorithms for multivariate anomaly detection using Isolation Forest, Local Outlier Factor, and One-Class SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb47082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_ml_features(df):\n",
    "    \"\"\"\n",
    "    Prepare features for machine learning anomaly detection\n",
    "    \"\"\"\n",
    "    print(\"ðŸ”§ PREPARING FEATURES FOR ML ANOMALY DETECTION\")\n",
    "    print(\"=\" * 55)\n",
    "    \n",
    "    # Select numerical features for ML models\n",
    "    numerical_features = [\n",
    "        'patient_age', 'total_billed_amount', 'insurance_paid_amount',\n",
    "        'patient_responsibility', 'length_of_stay', 'cost_per_day',\n",
    "        'payment_rate', 'days_since_service', 'insurance_efficiency',\n",
    "        'monthly_patient_total', 'provider_claim_count', 'provider_avg_billing'\n",
    "    ]\n",
    "    \n",
    "    # Select categorical features to encode\n",
    "    categorical_features = [\n",
    "        'procedure_name', 'insurance_provider', 'department',\n",
    "        'admission_type', 'patient_gender', 'age_group', 'cost_category'\n",
    "    ]\n",
    "    \n",
    "    # Create feature matrix\n",
    "    feature_df = df[numerical_features].copy()\n",
    "    \n",
    "    # One-hot encode categorical features (top categories only to avoid dimensionality explosion)\n",
    "    for cat_col in categorical_features:\n",
    "        # Get top 5 categories for each categorical feature\n",
    "        top_categories = df[cat_col].value_counts().head(5).index\n",
    "        for category in top_categories:\n",
    "            feature_df[f'{cat_col}_{category}'] = (df[cat_col] == category).astype(int)\n",
    "    \n",
    "    # Handle any missing values\n",
    "    feature_df = feature_df.fillna(feature_df.median())\n",
    "    \n",
    "    print(f\"âœ… Feature matrix prepared: {feature_df.shape}\")\n",
    "    print(f\"Features included: {list(feature_df.columns)}\")\n",
    "    \n",
    "    return feature_df\n",
    "\n",
    "def apply_ml_anomaly_detection(features_df, contamination=0.05):\n",
    "    \"\"\"\n",
    "    Apply multiple ML algorithms for anomaly detection\n",
    "    \"\"\"\n",
    "    print(f\"\\nðŸ¤– APPLYING ML ANOMALY DETECTION\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Expected contamination rate: {contamination*100:.1f}%\")\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features_df)\n",
    "    \n",
    "    ml_anomaly_results = {}\n",
    "    \n",
    "    # 1. Isolation Forest\n",
    "    print(f\"\\nðŸŒ² Isolation Forest:\")\n",
    "    iso_forest = IsolationForest(\n",
    "        contamination=contamination,\n",
    "        random_state=42,\n",
    "        n_estimators=100\n",
    "    )\n",
    "    iso_predictions = iso_forest.fit_predict(features_scaled)\n",
    "    iso_anomalies = iso_predictions == -1\n",
    "    ml_anomaly_results['isolation_forest'] = iso_anomalies\n",
    "    \n",
    "    print(f\"   Anomalies detected: {iso_anomalies.sum():,} ({iso_anomalies.mean()*100:.2f}%)\")\n",
    "    print(f\"   Anomaly scores range: {iso_forest.decision_function(features_scaled).min():.3f} to {iso_forest.decision_function(features_scaled).max():.3f}\")\n",
    "    \n",
    "    # 2. Local Outlier Factor\n",
    "    print(f\"\\nðŸŽ¯ Local Outlier Factor:\")\n",
    "    lof = LocalOutlierFactor(\n",
    "        contamination=contamination,\n",
    "        n_neighbors=20\n",
    "    )\n",
    "    lof_predictions = lof.fit_predict(features_scaled)\n",
    "    lof_anomalies = lof_predictions == -1\n",
    "    ml_anomaly_results['local_outlier_factor'] = lof_anomalies\n",
    "    \n",
    "    print(f\"   Anomalies detected: {lof_anomalies.sum():,} ({lof_anomalies.mean()*100:.2f}%)\")\n",
    "    print(f\"   Outlier scores range: {lof.negative_outlier_factor_.min():.3f} to {lof.negative_outlier_factor_.max():.3f}\")\n",
    "    \n",
    "    # 3. One-Class SVM\n",
    "    print(f\"\\nâš¡ One-Class SVM:\")\n",
    "    oc_svm = OneClassSVM(\n",
    "        nu=contamination,\n",
    "        kernel='rbf',\n",
    "        gamma='scale'\n",
    "    )\n",
    "    svm_predictions = oc_svm.fit_predict(features_scaled)\n",
    "    svm_anomalies = svm_predictions == -1\n",
    "    ml_anomaly_results['one_class_svm'] = svm_anomalies\n",
    "    \n",
    "    print(f\"   Anomalies detected: {svm_anomalies.sum():,} ({svm_anomalies.mean()*100:.2f}%)\")\n",
    "    \n",
    "    # Ensemble approach - majority voting\n",
    "    anomaly_counts = sum(ml_anomaly_results.values())\n",
    "    ensemble_anomalies = anomaly_counts >= 2  # At least 2 models agree\n",
    "    ml_anomaly_results['ml_ensemble'] = ensemble_anomalies\n",
    "    \n",
    "    print(f\"\\nðŸ¤ ML Ensemble (â‰¥2 models agree):\")\n",
    "    print(f\"   Anomalies detected: {ensemble_anomalies.sum():,} ({ensemble_anomalies.mean()*100:.2f}%)\")\n",
    "    \n",
    "    return ml_anomaly_results, scaler\n",
    "\n",
    "# Prepare features and apply ML anomaly detection\n",
    "features_df = prepare_ml_features(df_clean)\n",
    "ml_anomalies, feature_scaler = apply_ml_anomaly_detection(features_df, contamination=0.05)\n",
    "\n",
    "# Add ML anomaly flags to dataframe\n",
    "for method, anomalies in ml_anomalies.items():\n",
    "    df_clean[f'ml_anomaly_{method}'] = anomalies\n",
    "\n",
    "# Compare ML methods with statistical methods\n",
    "print(f\"\\nðŸ“Š COMPARISON: STATISTICAL vs ML METHODS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Method': ['Z-Score', 'IQR', 'Percentile', 'Modified Z-Score', 'Statistical Consensus',\n",
    "               'Isolation Forest', 'Local Outlier Factor', 'One-Class SVM', 'ML Ensemble'],\n",
    "    'Anomalies_Detected': [\n",
    "        df_clean['anomaly_z_score'].sum(),\n",
    "        df_clean['anomaly_iqr'].sum(),\n",
    "        df_clean['anomaly_percentile'].sum(),\n",
    "        df_clean['anomaly_modified_z'].sum(),\n",
    "        df_clean['anomaly_consensus'].sum(),\n",
    "        df_clean['ml_anomaly_isolation_forest'].sum(),\n",
    "        df_clean['ml_anomaly_local_outlier_factor'].sum(),\n",
    "        df_clean['ml_anomaly_one_class_svm'].sum(),\n",
    "        df_clean['ml_anomaly_ml_ensemble'].sum()\n",
    "    ]\n",
    "})\n",
    "comparison_df['Percentage'] = (comparison_df['Anomalies_Detected'] / len(df_clean) * 100).round(2)\n",
    "print(comparison_df)\n",
    "\n",
    "# Analyze overlap between statistical and ML methods\n",
    "print(f\"\\nðŸ”„ METHOD OVERLAP ANALYSIS\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "stat_anomalies = df_clean['anomaly_consensus']\n",
    "ml_ensemble_anomalies = df_clean['ml_anomaly_ml_ensemble']\n",
    "\n",
    "overlap = (stat_anomalies & ml_ensemble_anomalies).sum()\n",
    "only_statistical = (stat_anomalies & ~ml_ensemble_anomalies).sum()\n",
    "only_ml = (~stat_anomalies & ml_ensemble_anomalies).sum()\n",
    "\n",
    "print(f\"Both methods agree: {overlap:,} ({overlap/len(df_clean)*100:.2f}%)\")\n",
    "print(f\"Only statistical: {only_statistical:,} ({only_statistical/len(df_clean)*100:.2f}%)\")\n",
    "print(f\"Only ML methods: {only_ml:,} ({only_ml/len(df_clean)*100:.2f}%)\")\n",
    "\n",
    "# Create final anomaly flag combining best of both approaches\n",
    "df_clean['final_anomaly_flag'] = (\n",
    "    df_clean['anomaly_consensus'] | df_clean['ml_anomaly_ml_ensemble']\n",
    ")\n",
    "\n",
    "final_anomaly_count = df_clean['final_anomaly_flag'].sum()\n",
    "print(f\"\\nFinal anomaly detection: {final_anomaly_count:,} anomalies ({final_anomaly_count/len(df_clean)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nâœ… Machine learning anomaly detection completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905296f4",
   "metadata": {},
   "source": [
    "## 7. Cost Pattern Analysis ðŸ’°\n",
    "\n",
    "Deep dive analysis of cost patterns, trends, and relationships to identify billing insights and business intelligence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23f67d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost Analysis by Different Dimensions\n",
    "\n",
    "print(\"ðŸ’° COMPREHENSIVE COST PATTERN ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Cost by Procedure Type\n",
    "print(\"\\nðŸ“‹ COST ANALYSIS BY PROCEDURE TYPE\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "procedure_analysis = df_clean.groupby('procedure_name').agg({\n",
    "    'total_billed_amount': ['count', 'mean', 'median', 'sum', 'std'],\n",
    "    'insurance_paid_amount': 'mean',\n",
    "    'patient_responsibility': 'mean',\n",
    "    'length_of_stay': 'mean',\n",
    "    'final_anomaly_flag': 'sum'\n",
    "}).round(2)\n",
    "\n",
    "procedure_analysis.columns = [\n",
    "    'claim_count', 'avg_cost', 'median_cost', 'total_revenue', 'cost_std',\n",
    "    'avg_insurance_paid', 'avg_patient_responsibility', 'avg_length_stay', 'anomaly_count'\n",
    "]\n",
    "\n",
    "procedure_analysis['anomaly_rate'] = (\n",
    "    procedure_analysis['anomaly_count'] / procedure_analysis['claim_count'] * 100\n",
    ").round(2)\n",
    "\n",
    "procedure_analysis = procedure_analysis.sort_values('total_revenue', ascending=False)\n",
    "print(procedure_analysis)\n",
    "\n",
    "# 2. Cost by Insurance Provider\n",
    "print(f\"\\nðŸ¥ COST ANALYSIS BY INSURANCE PROVIDER\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "insurance_analysis = df_clean.groupby('insurance_provider').agg({\n",
    "    'total_billed_amount': ['count', 'mean', 'sum'],\n",
    "    'insurance_paid_amount': 'mean',\n",
    "    'payment_rate': 'mean',\n",
    "    'final_anomaly_flag': 'sum'\n",
    "}).round(2)\n",
    "\n",
    "insurance_analysis.columns = [\n",
    "    'claim_count', 'avg_billing', 'total_billing', 'avg_payment', 'avg_payment_rate', 'anomaly_count'\n",
    "]\n",
    "\n",
    "insurance_analysis['anomaly_rate'] = (\n",
    "    insurance_analysis['anomaly_count'] / insurance_analysis['claim_count'] * 100\n",
    ").round(2)\n",
    "\n",
    "insurance_analysis = insurance_analysis.sort_values('total_billing', ascending=False)\n",
    "print(insurance_analysis)\n",
    "\n",
    "# 3. Cost by Department\n",
    "print(f\"\\nðŸ¢ COST ANALYSIS BY DEPARTMENT\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "department_analysis = df_clean.groupby('department').agg({\n",
    "    'total_billed_amount': ['count', 'mean', 'sum'],\n",
    "    'provider_id': 'nunique',\n",
    "    'patient_id': 'nunique',\n",
    "    'final_anomaly_flag': 'sum'\n",
    "}).round(2)\n",
    "\n",
    "department_analysis.columns = [\n",
    "    'claim_count', 'avg_billing', 'total_billing', 'unique_providers', 'unique_patients', 'anomaly_count'\n",
    "]\n",
    "\n",
    "department_analysis = department_analysis.sort_values('total_billing', ascending=False)\n",
    "print(department_analysis)\n",
    "\n",
    "# 4. Temporal Cost Analysis\n",
    "print(f\"\\nðŸ“… TEMPORAL COST TRENDS\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "# Monthly trends\n",
    "monthly_trends = df_clean.groupby('service_month').agg({\n",
    "    'total_billed_amount': ['count', 'sum', 'mean'],\n",
    "    'final_anomaly_flag': 'sum'\n",
    "}).round(2)\n",
    "\n",
    "monthly_trends.columns = ['monthly_claims', 'monthly_revenue', 'avg_claim_amount', 'monthly_anomalies']\n",
    "monthly_trends['anomaly_rate'] = (\n",
    "    monthly_trends['monthly_anomalies'] / monthly_trends['monthly_claims'] * 100\n",
    ").round(2)\n",
    "\n",
    "print(\"Recent 6 months:\")\n",
    "print(monthly_trends.tail(6))\n",
    "\n",
    "# 5. Provider Performance Analysis\n",
    "print(f\"\\nðŸ‘¨â€âš•ï¸ TOP PROVIDER PERFORMANCE ANALYSIS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Top providers by revenue\n",
    "top_providers = df_clean.groupby('provider_id').agg({\n",
    "    'total_billed_amount': ['count', 'sum', 'mean'],\n",
    "    'patient_id': 'nunique',\n",
    "    'final_anomaly_flag': 'sum'\n",
    "}).round(2)\n",
    "\n",
    "top_providers.columns = ['total_claims', 'total_revenue', 'avg_claim_amount', 'unique_patients', 'anomaly_count']\n",
    "top_providers['anomaly_rate'] = (\n",
    "    top_providers['anomaly_count'] / top_providers['total_claims'] * 100\n",
    ").round(2)\n",
    "top_providers['revenue_per_patient'] = (\n",
    "    top_providers['total_revenue'] / top_providers['unique_patients']\n",
    ").round(2)\n",
    "\n",
    "# Filter providers with significant volume (>50 claims)\n",
    "significant_providers = top_providers[top_providers['total_claims'] >= 50]\n",
    "print(\"Top 10 providers by revenue (minimum 50 claims):\")\n",
    "print(significant_providers.sort_values('total_revenue', ascending=False).head(10))\n",
    "\n",
    "# 6. Patient Demographics and Cost Analysis\n",
    "print(f\"\\nðŸ‘¥ DEMOGRAPHIC COST ANALYSIS\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Age group analysis\n",
    "age_analysis = df_clean.groupby('age_group').agg({\n",
    "    'total_billed_amount': ['count', 'mean', 'sum'],\n",
    "    'insurance_paid_amount': 'mean',\n",
    "    'final_anomaly_flag': 'sum'\n",
    "}).round(2)\n",
    "\n",
    "age_analysis.columns = ['claim_count', 'avg_cost', 'total_cost', 'avg_insurance_paid', 'anomaly_count']\n",
    "age_analysis['anomaly_rate'] = (age_analysis['anomaly_count'] / age_analysis['claim_count'] * 100).round(2)\n",
    "\n",
    "print(\"Cost analysis by age group:\")\n",
    "print(age_analysis)\n",
    "\n",
    "# Gender analysis\n",
    "gender_analysis = df_clean.groupby('patient_gender').agg({\n",
    "    'total_billed_amount': ['count', 'mean', 'sum'],\n",
    "    'final_anomaly_flag': 'sum'\n",
    "}).round(2)\n",
    "\n",
    "gender_analysis.columns = ['claim_count', 'avg_cost', 'total_cost', 'anomaly_count']\n",
    "gender_analysis['anomaly_rate'] = (gender_analysis['anomaly_count'] / gender_analysis['claim_count'] * 100).round(2)\n",
    "\n",
    "print(f\"\\nCost analysis by gender:\")\n",
    "print(gender_analysis)\n",
    "\n",
    "# 7. High-Value Claims Analysis\n",
    "print(f\"\\nðŸ’Ž HIGH-VALUE CLAIMS ANALYSIS\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Define high-value threshold (top 5%)\n",
    "high_value_threshold = df_clean['total_billed_amount'].quantile(0.95)\n",
    "high_value_claims = df_clean[df_clean['total_billed_amount'] >= high_value_threshold]\n",
    "\n",
    "print(f\"High-value threshold: ${high_value_threshold:,.2f}\")\n",
    "print(f\"High-value claims: {len(high_value_claims):,} ({len(high_value_claims)/len(df_clean)*100:.2f}%)\")\n",
    "print(f\"Revenue from high-value claims: ${high_value_claims['total_billed_amount'].sum():,.2f}\")\n",
    "print(f\"Percentage of total revenue: {high_value_claims['total_billed_amount'].sum()/df_clean['total_billed_amount'].sum()*100:.1f}%\")\n",
    "\n",
    "high_value_summary = high_value_claims.groupby(['procedure_name', 'department']).agg({\n",
    "    'claim_id': 'count',\n",
    "    'total_billed_amount': ['mean', 'max'],\n",
    "    'final_anomaly_flag': 'sum'\n",
    "}).round(2)\n",
    "\n",
    "print(f\"\\nHigh-value claims by procedure and department:\")\n",
    "print(high_value_summary.head(10))\n",
    "\n",
    "print(f\"\\nâœ… Cost pattern analysis completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca3ff7f",
   "metadata": {},
   "source": [
    "## 8. Visualization of Anomalies and Insights ðŸ“Š\n",
    "\n",
    "Creating comprehensive visualizations to display anomalies, cost patterns, and business insights for stakeholder communication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d431e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up plotting parameters\n",
    "plt.style.use('default')\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f']\n",
    "\n",
    "print(\"ðŸ“Š CREATING COMPREHENSIVE VISUALIZATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Anomaly Detection Overview\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "\n",
    "# Anomaly distribution by method\n",
    "anomaly_methods = ['Z-Score', 'IQR', 'Percentile', 'Statistical Consensus', \n",
    "                   'Isolation Forest', 'LOF', 'One-Class SVM', 'ML Ensemble', 'Final Combined']\n",
    "anomaly_counts = [\n",
    "    df_clean['anomaly_z_score'].sum(),\n",
    "    df_clean['anomaly_iqr'].sum(), \n",
    "    df_clean['anomaly_percentile'].sum(),\n",
    "    df_clean['anomaly_consensus'].sum(),\n",
    "    df_clean['ml_anomaly_isolation_forest'].sum(),\n",
    "    df_clean['ml_anomaly_local_outlier_factor'].sum(),\n",
    "    df_clean['ml_anomaly_one_class_svm'].sum(),\n",
    "    df_clean['ml_anomaly_ml_ensemble'].sum(),\n",
    "    df_clean['final_anomaly_flag'].sum()\n",
    "]\n",
    "\n",
    "axes[0,0].bar(range(len(anomaly_methods)), anomaly_counts, color=colors)\n",
    "axes[0,0].set_xticks(range(len(anomaly_methods)))\n",
    "axes[0,0].set_xticklabels(anomaly_methods, rotation=45, ha='right')\n",
    "axes[0,0].set_title('Anomalies Detected by Different Methods', fontsize=14, fontweight='bold')\n",
    "axes[0,0].set_ylabel('Number of Anomalies')\n",
    "\n",
    "# Cost distribution with anomalies highlighted\n",
    "normal_costs = df_clean[~df_clean['final_anomaly_flag']]['total_billed_amount']\n",
    "anomaly_costs = df_clean[df_clean['final_anomaly_flag']]['total_billed_amount']\n",
    "\n",
    "axes[0,1].hist(normal_costs, bins=50, alpha=0.7, label='Normal Claims', color='lightblue', density=True)\n",
    "axes[0,1].hist(anomaly_costs, bins=50, alpha=0.7, label='Anomalous Claims', color='red', density=True)\n",
    "axes[0,1].set_xlabel('Total Billed Amount ($)')\n",
    "axes[0,1].set_ylabel('Density')\n",
    "axes[0,1].set_title('Cost Distribution: Normal vs Anomalous Claims', fontsize=14, fontweight='bold')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].set_xlim(0, 20000)  # Focus on main distribution\n",
    "\n",
    "# Anomalies by procedure type\n",
    "procedure_anomalies = df_clean[df_clean['final_anomaly_flag']].groupby('procedure_name').size().sort_values(ascending=True)\n",
    "axes[1,0].barh(range(len(procedure_anomalies)), procedure_anomalies.values, color='coral')\n",
    "axes[1,0].set_yticks(range(len(procedure_anomalies)))\n",
    "axes[1,0].set_yticklabels(procedure_anomalies.index)\n",
    "axes[1,0].set_title('Anomalies by Procedure Type', fontsize=14, fontweight='bold')\n",
    "axes[1,0].set_xlabel('Number of Anomalies')\n",
    "\n",
    "# Temporal anomaly pattern\n",
    "monthly_anomalies = df_clean[df_clean['final_anomaly_flag']].groupby('service_month').size()\n",
    "axes[1,1].plot(range(len(monthly_anomalies)), monthly_anomalies.values, marker='o', linewidth=2, markersize=6, color='darkred')\n",
    "axes[1,1].set_xticks(range(0, len(monthly_anomalies), 3))\n",
    "axes[1,1].set_xticklabels([monthly_anomalies.index[i] for i in range(0, len(monthly_anomalies), 3)], rotation=45)\n",
    "axes[1,1].set_title('Monthly Anomaly Trends', fontsize=14, fontweight='bold')\n",
    "axes[1,1].set_ylabel('Number of Anomalies')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Cost Pattern Analysis Visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "\n",
    "# Average cost by procedure\n",
    "procedure_costs = df_clean.groupby('procedure_name')['total_billed_amount'].mean().sort_values(ascending=True)\n",
    "axes[0,0].barh(range(len(procedure_costs)), procedure_costs.values, color='skyblue')\n",
    "axes[0,0].set_yticks(range(len(procedure_costs)))\n",
    "axes[0,0].set_yticklabels(procedure_costs.index)\n",
    "axes[0,0].set_title('Average Cost by Procedure Type', fontsize=14, fontweight='bold')\n",
    "axes[0,0].set_xlabel('Average Cost ($)')\n",
    "\n",
    "# Insurance payment efficiency\n",
    "insurance_efficiency = df_clean.groupby('insurance_provider')['payment_rate'].mean().sort_values(ascending=False)\n",
    "axes[0,1].bar(range(len(insurance_efficiency)), insurance_efficiency.values, color='lightgreen')\n",
    "axes[0,1].set_xticks(range(len(insurance_efficiency)))\n",
    "axes[0,1].set_xticklabels(insurance_efficiency.index, rotation=45, ha='right')\n",
    "axes[0,1].set_title('Insurance Payment Efficiency', fontsize=14, fontweight='bold')\n",
    "axes[0,1].set_ylabel('Average Payment Rate')\n",
    "axes[0,1].set_ylim(0, 1)\n",
    "\n",
    "# Cost by age group\n",
    "age_costs = df_clean.groupby('age_group')['total_billed_amount'].mean()\n",
    "age_order = ['Pediatric (0-17)', 'Young Adult (18-34)', 'Middle Age (35-49)', 'Older Adult (50-64)', 'Senior (65+)']\n",
    "age_costs_ordered = [age_costs[age] for age in age_order if age in age_costs.index]\n",
    "axes[1,0].bar(range(len(age_costs_ordered)), age_costs_ordered, color='orange')\n",
    "axes[1,0].set_xticks(range(len(age_costs_ordered)))\n",
    "axes[1,0].set_xticklabels([age for age in age_order if age in age_costs.index], rotation=45, ha='right')\n",
    "axes[1,0].set_title('Average Cost by Age Group', fontsize=14, fontweight='bold')\n",
    "axes[1,0].set_ylabel('Average Cost ($)')\n",
    "\n",
    "# Monthly revenue trends\n",
    "monthly_revenue = df_clean.groupby('service_month')['total_billed_amount'].sum()\n",
    "axes[1,1].plot(range(len(monthly_revenue)), monthly_revenue.values, marker='s', linewidth=3, markersize=8, color='purple')\n",
    "axes[1,1].set_xticks(range(0, len(monthly_revenue), 3))\n",
    "axes[1,1].set_xticklabels([monthly_revenue.index[i] for i in range(0, len(monthly_revenue), 3)], rotation=45)\n",
    "axes[1,1].set_title('Monthly Revenue Trends', fontsize=14, fontweight='bold')\n",
    "axes[1,1].set_ylabel('Total Revenue ($)')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Interactive Plotly Visualizations\n",
    "print(\"\\nðŸŽ¨ Creating Interactive Plotly Visualizations...\")\n",
    "\n",
    "# Interactive scatter plot: Cost vs Age with anomalies highlighted\n",
    "fig_scatter = px.scatter(\n",
    "    df_clean.sample(5000),  # Sample for performance\n",
    "    x='patient_age',\n",
    "    y='total_billed_amount',\n",
    "    color='final_anomaly_flag',\n",
    "    hover_data=['procedure_name', 'insurance_provider', 'department'],\n",
    "    title='Healthcare Costs by Patient Age (Anomalies Highlighted)',\n",
    "    labels={'patient_age': 'Patient Age', 'total_billed_amount': 'Total Billed Amount ($)'}\n",
    ")\n",
    "fig_scatter.update_layout(height=600)\n",
    "fig_scatter.show()\n",
    "\n",
    "# Box plot of costs by procedure type\n",
    "fig_box = px.box(\n",
    "    df_clean,\n",
    "    x='procedure_name',\n",
    "    y='total_billed_amount',\n",
    "    title='Cost Distribution by Procedure Type',\n",
    "    labels={'procedure_name': 'Procedure Type', 'total_billed_amount': 'Total Billed Amount ($)'}\n",
    ")\n",
    "fig_box.update_xaxes(tickangle=45)\n",
    "fig_box.update_layout(height=600)\n",
    "fig_box.show()\n",
    "\n",
    "# Heatmap of anomaly rates by procedure and department\n",
    "anomaly_heatmap = df_clean.groupby(['procedure_name', 'department']).agg({\n",
    "    'final_anomaly_flag': 'mean',\n",
    "    'claim_id': 'count'\n",
    "}).reset_index()\n",
    "anomaly_heatmap = anomaly_heatmap[anomaly_heatmap['claim_id'] >= 10]  # Filter for significance\n",
    "\n",
    "heatmap_pivot = anomaly_heatmap.pivot(index='procedure_name', columns='department', values='final_anomaly_flag')\n",
    "\n",
    "fig_heatmap = px.imshow(\n",
    "    heatmap_pivot,\n",
    "    title='Anomaly Rate Heatmap: Procedure Type vs Department',\n",
    "    labels={'color': 'Anomaly Rate'},\n",
    "    aspect='auto'\n",
    ")\n",
    "fig_heatmap.update_layout(height=600)\n",
    "fig_heatmap.show()\n",
    "\n",
    "# 4. Provider Performance Analysis Visualization\n",
    "print(\"\\nðŸ‘¨â€âš•ï¸ Provider Performance Visualizations...\")\n",
    "\n",
    "# Top providers by anomaly rate (minimum 30 claims)\n",
    "provider_performance = df_clean.groupby('provider_id').agg({\n",
    "    'total_billed_amount': ['count', 'sum', 'mean'],\n",
    "    'final_anomaly_flag': 'sum'\n",
    "}).round(2)\n",
    "\n",
    "provider_performance.columns = ['claim_count', 'total_revenue', 'avg_claim_amount', 'anomaly_count']\n",
    "provider_performance['anomaly_rate'] = (\n",
    "    provider_performance['anomaly_count'] / provider_performance['claim_count']\n",
    ").round(3)\n",
    "\n",
    "# Filter for providers with sufficient volume\n",
    "qualified_providers = provider_performance[provider_performance['claim_count'] >= 30]\n",
    "high_anomaly_providers = qualified_providers.nlargest(15, 'anomaly_rate')\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# High anomaly rate providers\n",
    "ax1.barh(range(len(high_anomaly_providers)), high_anomaly_providers['anomaly_rate'], color='red', alpha=0.7)\n",
    "ax1.set_yticks(range(len(high_anomaly_providers)))\n",
    "ax1.set_yticklabels(high_anomaly_providers.index)\n",
    "ax1.set_title('Providers with Highest Anomaly Rates (â‰¥30 claims)', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Anomaly Rate')\n",
    "\n",
    "# Provider revenue vs anomaly rate scatter\n",
    "ax2.scatter(qualified_providers['total_revenue'], qualified_providers['anomaly_rate'], \n",
    "           alpha=0.6, s=qualified_providers['claim_count']*2, color='blue')\n",
    "ax2.set_xlabel('Total Revenue ($)')\n",
    "ax2.set_ylabel('Anomaly Rate')\n",
    "ax2.set_title('Provider Revenue vs Anomaly Rate\\n(Bubble size = Claim Count)', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… All visualizations created successfully!\")\n",
    "print(\"ðŸ“Š Interactive plots displayed above provide detailed insights for stakeholder analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fc9a76",
   "metadata": {},
   "source": [
    "## 9. Export Results for Power BI Integration ðŸ“¤\n",
    "\n",
    "Preparing and exporting cleaned data, anomaly flags, and summary statistics in formats optimized for Power BI dashboard creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1dc0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data exports for Power BI Dashboard\n",
    "import os\n",
    "\n",
    "# Create exports directory if it doesn't exist\n",
    "export_dir = '../data/powerbi_exports'\n",
    "os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "print(\"ðŸ“¤ PREPARING DATA EXPORTS FOR POWER BI\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Main Dataset with Anomaly Flags\n",
    "print(\"1ï¸âƒ£ Exporting main dataset with anomaly flags...\")\n",
    "\n",
    "# Select key columns for Power BI\n",
    "powerbi_columns = [\n",
    "    'claim_id', 'patient_id', 'patient_age', 'patient_gender', 'age_group',\n",
    "    'service_date', 'service_month', 'service_quarter', 'service_year', 'day_of_week',\n",
    "    'procedure_name', 'procedure_code', 'procedure_complexity', 'primary_diagnosis',\n",
    "    'department', 'provider_id', 'insurance_provider', 'admission_type', 'facility_type',\n",
    "    'total_billed_amount', 'insurance_paid_amount', 'patient_responsibility',\n",
    "    'payment_rate', 'insurance_efficiency', 'length_of_stay', 'cost_per_day',\n",
    "    'cost_category', 'claim_status', 'final_anomaly_flag', 'is_potential_duplicate',\n",
    "    'monthly_patient_total', 'provider_avg_billing', 'provider_claim_count'\n",
    "]\n",
    "\n",
    "powerbi_main = df_clean[powerbi_columns].copy()\n",
    "\n",
    "# Add readable anomaly status\n",
    "powerbi_main['anomaly_status'] = powerbi_main['final_anomaly_flag'].map({True: 'Anomalous', False: 'Normal'})\n",
    "\n",
    "# Export main dataset\n",
    "powerbi_main.to_csv(f'{export_dir}/healthcare_billing_main.csv', index=False)\n",
    "print(f\"   âœ… Exported: healthcare_billing_main.csv ({len(powerbi_main):,} records)\")\n",
    "\n",
    "# 2. Anomaly Summary Table\n",
    "print(\"\\n2ï¸âƒ£ Creating anomaly summary table...\")\n",
    "\n",
    "anomaly_summary = pd.DataFrame({\n",
    "    'Detection_Method': [\n",
    "        'Z-Score', 'IQR', 'Percentile', 'Modified Z-Score', 'Statistical Consensus',\n",
    "        'Isolation Forest', 'Local Outlier Factor', 'One-Class SVM', 'ML Ensemble', 'Final Combined'\n",
    "    ],\n",
    "    'Anomalies_Detected': [\n",
    "        df_clean['anomaly_z_score'].sum(),\n",
    "        df_clean['anomaly_iqr'].sum(),\n",
    "        df_clean['anomaly_percentile'].sum(),\n",
    "        df_clean['anomaly_modified_z'].sum(),\n",
    "        df_clean['anomaly_consensus'].sum(),\n",
    "        df_clean['ml_anomaly_isolation_forest'].sum(),\n",
    "        df_clean['ml_anomaly_local_outlier_factor'].sum(),\n",
    "        df_clean['ml_anomaly_one_class_svm'].sum(),\n",
    "        df_clean['ml_anomaly_ml_ensemble'].sum(),\n",
    "        df_clean['final_anomaly_flag'].sum()\n",
    "    ]\n",
    "})\n",
    "anomaly_summary['Percentage'] = (anomaly_summary['Anomalies_Detected'] / len(df_clean) * 100).round(2)\n",
    "anomaly_summary['Method_Type'] = ['Statistical'] * 5 + ['Machine Learning'] * 4 + ['Combined']\n",
    "\n",
    "anomaly_summary.to_csv(f'{export_dir}/anomaly_detection_summary.csv', index=False)\n",
    "print(f\"   âœ… Exported: anomaly_detection_summary.csv\")\n",
    "\n",
    "# 3. Procedure Analysis Summary\n",
    "print(\"\\n3ï¸âƒ£ Creating procedure analysis summary...\")\n",
    "\n",
    "procedure_summary = df_clean.groupby(['procedure_name', 'procedure_complexity']).agg({\n",
    "    'claim_id': 'count',\n",
    "    'total_billed_amount': ['mean', 'median', 'sum', 'std'],\n",
    "    'insurance_paid_amount': 'mean',\n",
    "    'length_of_stay': 'mean',\n",
    "    'final_anomaly_flag': 'sum',\n",
    "    'patient_id': 'nunique'\n",
    "}).round(2)\n",
    "\n",
    "procedure_summary.columns = [\n",
    "    'total_claims', 'avg_cost', 'median_cost', 'total_revenue', 'cost_std',\n",
    "    'avg_insurance_paid', 'avg_length_stay', 'anomaly_count', 'unique_patients'\n",
    "]\n",
    "\n",
    "procedure_summary['anomaly_rate'] = (\n",
    "    procedure_summary['anomaly_count'] / procedure_summary['total_claims'] * 100\n",
    ").round(2)\n",
    "\n",
    "procedure_summary = procedure_summary.reset_index()\n",
    "procedure_summary.to_csv(f'{export_dir}/procedure_analysis_summary.csv', index=False)\n",
    "print(f\"   âœ… Exported: procedure_analysis_summary.csv\")\n",
    "\n",
    "# 4. Provider Performance Summary\n",
    "print(\"\\n4ï¸âƒ£ Creating provider performance summary...\")\n",
    "\n",
    "provider_summary = df_clean.groupby('provider_id').agg({\n",
    "    'claim_id': 'count',\n",
    "    'total_billed_amount': ['sum', 'mean'],\n",
    "    'patient_id': 'nunique',\n",
    "    'final_anomaly_flag': 'sum',\n",
    "    'department': lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else 'Unknown'\n",
    "}).round(2)\n",
    "\n",
    "provider_summary.columns = [\n",
    "    'total_claims', 'total_revenue', 'avg_claim_amount', 'unique_patients', \n",
    "    'anomaly_count', 'primary_department'\n",
    "]\n",
    "\n",
    "provider_summary['anomaly_rate'] = (\n",
    "    provider_summary['anomaly_count'] / provider_summary['total_claims'] * 100\n",
    ").round(2)\n",
    "\n",
    "provider_summary['revenue_per_patient'] = (\n",
    "    provider_summary['total_revenue'] / provider_summary['unique_patients']\n",
    ").round(2)\n",
    "\n",
    "provider_summary = provider_summary.reset_index()\n",
    "\n",
    "# Add performance categories\n",
    "def categorize_performance(row):\n",
    "    if row['anomaly_rate'] > 10:\n",
    "        return 'High Risk'\n",
    "    elif row['anomaly_rate'] > 5:\n",
    "        return 'Medium Risk'\n",
    "    else:\n",
    "        return 'Low Risk'\n",
    "\n",
    "provider_summary['risk_category'] = provider_summary.apply(categorize_performance, axis=1)\n",
    "\n",
    "provider_summary.to_csv(f'{export_dir}/provider_performance_summary.csv', index=False)\n",
    "print(f\"   âœ… Exported: provider_performance_summary.csv\")\n",
    "\n",
    "# 5. Monthly Trends Summary\n",
    "print(\"\\n5ï¸âƒ£ Creating monthly trends summary...\")\n",
    "\n",
    "monthly_summary = df_clean.groupby(['service_month', 'service_year', 'service_quarter']).agg({\n",
    "    'claim_id': 'count',\n",
    "    'total_billed_amount': ['sum', 'mean'],\n",
    "    'insurance_paid_amount': 'sum',\n",
    "    'patient_responsibility': 'sum',\n",
    "    'final_anomaly_flag': 'sum',\n",
    "    'patient_id': 'nunique',\n",
    "    'provider_id': 'nunique'\n",
    "}).round(2)\n",
    "\n",
    "monthly_summary.columns = [\n",
    "    'monthly_claims', 'monthly_revenue', 'avg_claim_amount', 'monthly_insurance_paid',\n",
    "    'monthly_patient_responsibility', 'monthly_anomalies', 'unique_patients', 'unique_providers'\n",
    "]\n",
    "\n",
    "monthly_summary['anomaly_rate'] = (\n",
    "    monthly_summary['monthly_anomalies'] / monthly_summary['monthly_claims'] * 100\n",
    ").round(2)\n",
    "\n",
    "monthly_summary = monthly_summary.reset_index()\n",
    "monthly_summary.to_csv(f'{export_dir}/monthly_trends_summary.csv', index=False)\n",
    "print(f\"   âœ… Exported: monthly_trends_summary.csv\")\n",
    "\n",
    "# 6. Insurance Analysis Summary\n",
    "print(\"\\n6ï¸âƒ£ Creating insurance analysis summary...\")\n",
    "\n",
    "insurance_summary = df_clean.groupby('insurance_provider').agg({\n",
    "    'claim_id': 'count',\n",
    "    'total_billed_amount': ['sum', 'mean'],\n",
    "    'insurance_paid_amount': ['sum', 'mean'],\n",
    "    'payment_rate': 'mean',\n",
    "    'final_anomaly_flag': 'sum',\n",
    "    'patient_id': 'nunique'\n",
    "}).round(2)\n",
    "\n",
    "insurance_summary.columns = [\n",
    "    'total_claims', 'total_billed', 'avg_billed_per_claim', 'total_paid', \n",
    "    'avg_paid_per_claim', 'avg_payment_rate', 'anomaly_count', 'unique_patients'\n",
    "]\n",
    "\n",
    "insurance_summary['anomaly_rate'] = (\n",
    "    insurance_summary['anomaly_count'] / insurance_summary['total_claims'] * 100\n",
    ").round(2)\n",
    "\n",
    "insurance_summary = insurance_summary.reset_index()\n",
    "insurance_summary.to_csv(f'{export_dir}/insurance_analysis_summary.csv', index=False)\n",
    "print(f\"   âœ… Exported: insurance_analysis_summary.csv\")\n",
    "\n",
    "# 7. Key Performance Indicators (KPIs)\n",
    "print(\"\\n7ï¸âƒ£ Creating KPI summary...\")\n",
    "\n",
    "kpi_summary = pd.DataFrame({\n",
    "    'KPI_Name': [\n",
    "        'Total Claims',\n",
    "        'Total Revenue',\n",
    "        'Average Claim Amount',\n",
    "        'Total Anomalies',\n",
    "        'Anomaly Rate (%)',\n",
    "        'Unique Patients',\n",
    "        'Unique Providers',\n",
    "        'Average Length of Stay',\n",
    "        'Average Payment Rate',\n",
    "        'High Value Claims (>95th percentile)',\n",
    "        'Potential Duplicates'\n",
    "    ],\n",
    "    'Value': [\n",
    "        len(df_clean),\n",
    "        df_clean['total_billed_amount'].sum(),\n",
    "        df_clean['total_billed_amount'].mean(),\n",
    "        df_clean['final_anomaly_flag'].sum(),\n",
    "        df_clean['final_anomaly_flag'].mean() * 100,\n",
    "        df_clean['patient_id'].nunique(),\n",
    "        df_clean['provider_id'].nunique(),\n",
    "        df_clean['length_of_stay'].mean(),\n",
    "        df_clean['payment_rate'].mean(),\n",
    "        len(df_clean[df_clean['total_billed_amount'] > df_clean['total_billed_amount'].quantile(0.95)]),\n",
    "        df_clean['is_potential_duplicate'].sum()\n",
    "    ]\n",
    "})\n",
    "\n",
    "kpi_summary['Formatted_Value'] = kpi_summary.apply(lambda row: \n",
    "    f\"${row['Value']:,.2f}\" if 'Revenue' in row['KPI_Name'] or 'Amount' in row['KPI_Name'] \n",
    "    else f\"{row['Value']:,.0f}\" if row['Value'] > 1 \n",
    "    else f\"{row['Value']:.2f}\", axis=1)\n",
    "\n",
    "kpi_summary.to_csv(f'{export_dir}/kpi_summary.csv', index=False)\n",
    "print(f\"   âœ… Exported: kpi_summary.csv\")\n",
    "\n",
    "# 8. Create Data Dictionary\n",
    "print(\"\\n8ï¸âƒ£ Creating data dictionary...\")\n",
    "\n",
    "data_dictionary = pd.DataFrame({\n",
    "    'Column_Name': powerbi_main.columns,\n",
    "    'Data_Type': [str(dtype) for dtype in powerbi_main.dtypes],\n",
    "    'Description': [\n",
    "        'Unique claim identifier',\n",
    "        'Unique patient identifier', \n",
    "        'Patient age in years',\n",
    "        'Patient gender',\n",
    "        'Age group category',\n",
    "        'Date of service',\n",
    "        'Service month (YYYY-MM)',\n",
    "        'Service quarter (1-4)',\n",
    "        'Service year',\n",
    "        'Day of week for service',\n",
    "        'Medical procedure name',\n",
    "        'Medical procedure code',\n",
    "        'Procedure complexity level',\n",
    "        'Primary diagnosis code',\n",
    "        'Hospital department',\n",
    "        'Healthcare provider ID',\n",
    "        'Insurance company name',\n",
    "        'Type of admission',\n",
    "        'Healthcare facility type',\n",
    "        'Total amount billed',\n",
    "        'Amount paid by insurance',\n",
    "        'Amount owed by patient',\n",
    "        'Insurance payment rate (0-1)',\n",
    "        'Insurance coverage efficiency',\n",
    "        'Length of hospital stay in days',\n",
    "        'Cost per day of stay',\n",
    "        'Cost category classification',\n",
    "        'Claim processing status',\n",
    "        'Final anomaly detection flag',\n",
    "        'Potential duplicate claim flag',\n",
    "        'Monthly billing total per patient',\n",
    "        'Provider average billing amount',\n",
    "        'Provider total claim count',\n",
    "        'Anomaly status (Anomalous/Normal)'\n",
    "    ]\n",
    "})\n",
    "\n",
    "data_dictionary.to_csv(f'{export_dir}/data_dictionary.csv', index=False)\n",
    "print(f\"   âœ… Exported: data_dictionary.csv\")\n",
    "\n",
    "# Summary of exports\n",
    "print(f\"\\nðŸ“‹ EXPORT SUMMARY\")\n",
    "print(\"=\" * 25)\n",
    "print(f\"Export directory: {export_dir}\")\n",
    "print(f\"Files created: 8\")\n",
    "print(f\"Main dataset records: {len(powerbi_main):,}\")\n",
    "print(f\"Date range: {df_clean['service_date'].min().date()} to {df_clean['service_date'].max().date()}\")\n",
    "print(f\"Total anomalies flagged: {df_clean['final_anomaly_flag'].sum():,}\")\n",
    "\n",
    "print(f\"\\nâœ… All exports completed successfully!\")\n",
    "print(f\"ðŸŽ¯ Data is now ready for Power BI dashboard creation!\")\n",
    "print(f\"ðŸ’¡ Use the KPI summary and data dictionary to build meaningful visualizations.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
